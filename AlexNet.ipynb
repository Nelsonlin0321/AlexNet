{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "图像识别一直是机器学习中非常重要的一个研究内容, 2010年开始举办的[ILSVRC](http://image-net.org/challenges/LSVRC/)比赛更是吸引了无数的团队. 这个比赛基于一个百万量级的图片数据集, 提出一个图像1000分类的挑战. 前两年在比赛中脱颖而出的都是经过人工挑选特征, 再通过`SVM`或者`随机森林`这样在过去十几年中非常成熟的机器学习方法进行分类的算法. \n",
    "\n",
    "在2012年, 由 [Alex Krizhevsky](https://www.cs.toronto.edu/~kriz/), [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/)提出了一种使用卷积神经网络的方法, 以 [0.85](http://image-net.org/challenges/LSVRC/2012/results.html#abstract) 的`top-5`正确率一举获得当年分类比赛的冠军, 超越使用传统方法的第二名10个百分点, 震惊了当时的学术界, 从此开启了人工智能领域的新篇章.\n",
    "\n",
    "这次的课程我们就来复现一次`AlexNet`, 首先来看它的网络结构\n",
    "\n",
    "<img src=\"https://image.ibb.co/mP9C0x/alexnet_arch.png\">\n",
    "\n",
    "可以看出`AlexNet`就是几个卷积池化堆叠后连接几个全连接层, 下面就让我们来尝试仿照这个结构来解决[cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)分类问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CIFAR10`\n",
    "\n",
    "[![cifar10](https://image.ibb.co/n885Lx/cifar10.png)](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "这是一个包含60000张$32\\times32$图片的数据库, 包含50000张训练集和10000张测试集, 在这里我们提供一个脚本帮助我们读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 下载数据\n",
    "data_dir = 'cifar10_data'\n",
    "# cifar10_input.maybe_download(data_dir='cifar10_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取训练集\n",
    "# 在使用随机梯度下降法的时候, 训练集要求打乱样本\n",
    "train_imgs, train_labels = cifar10_input.inputs(eval_data=False, \n",
    "                                                data_dir='%s/cifar-10-batches-bin/' % data_dir, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True)\n",
    "\n",
    "# 获取测试集\n",
    "# 测试集不需要打乱样本\n",
    "val_imgs, val_labels = cifar10_input.inputs(eval_data=True, \n",
    "                                            data_dir='%s/cifar-10-batches-bin/' % data_dir, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN # 训练样本的个数\n",
    "val_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL       # 测试样本的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the weight and bias Creation funciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_weight(shape,stddev = 5e-1):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(shape=shape,initializer=init,name = 'weight')\n",
    "\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(shape=shape,initializer=init,name = 'bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建卷积层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# out_depth 一个images 需要多少个卷积核\n",
    "def conv(x,ksize,out_depth,strides,padding = 'SAME', act = tf.nn.relu,scope = 'conv_layer',reuse = None):\n",
    "    \n",
    "    # 这里默认数据是NHWC输入的\n",
    "    # shape  = ( bacth, height, weight, channel)\n",
    "    ## 获取输入数据的channels\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    ### 创建卷积层 变量域及计算\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        # shape = (height, weight,in_depth,out_depth),\n",
    "        # in_depth 表示channel 的个数，表示卷积核需要复制多少个 与 图片进行各自卷积\n",
    "        #out_depth  等于 卷积核个数,表示需要每一个channle需要多少个卷积核进行卷积.\n",
    "        # 构造卷积核\n",
    "        shape = ksize+[in_depth,out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(shape)\n",
    "\n",
    "        #  定义步长：\n",
    "        strides = [1,strides[0],strides[1],1]\n",
    "\n",
    "        # 进行卷积\n",
    "        conv = tf.nn.conv2d(input=x,filter=kernel,strides=strides,padding=padding,name='conv')\n",
    "\n",
    "        ## 构造偏置项目\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth]) ### 输出channels 的多少， 一个channel 一张图片\n",
    "\n",
    "        precat = tf.nn.bias_add(conv,bias)\n",
    "\n",
    "        ## 添加激活层\n",
    "\n",
    "        out = act(precat)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x,ksize,strides,padding,name='pool_layer'):\n",
    "    out = tf.nn.max_pool(value=x,ksize=[1,ksize[0],ksize[1],1],\n",
    "                         strides=[1, strides[0],strides[1], 1],\n",
    "                         padding=padding,name=name)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### out_deth   输出多少个维度\n",
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    \"\"\"构造一个全连接层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        out_depth: 输出向量的维数\n",
    "        act: 激活函数, 默认是`tf.nn.relu`\n",
    "        scope: 名称域, 默认是`fully_connect`\n",
    "        reuse: 是否需要重用\n",
    "    \"\"\"\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    # 构造全连接层的参数\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 构造权重\n",
    "        with tf.variable_scope('weight'):\n",
    "            weight = variable_weight([in_depth, out_depth])\n",
    "            \n",
    "        # 构造偏置项\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        \n",
    "        # 一个线性函数\n",
    "        fc = tf.nn.bias_add(tf.matmul(x, weight), bias, name='fc')\n",
    "        \n",
    "        # 激活函数作用\n",
    "        out = act(fc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面这些上层函数之后, 我们就可以轻松构建`AlexNet`了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alexnet(inputs, reuse=tf.AUTO_REUSE):\n",
    "    print(inputs.get_shape().as_list())\n",
    "    \"\"\"构建 Alexnet 的前向传播\n",
    "    Args:\n",
    "        inpus: 输入\n",
    "        reuse: 是否需要重用\n",
    "        \n",
    "    Return:\n",
    "        net: alexnet的结果\n",
    "    \"\"\"\n",
    "    # 首先我们声明一个变量域`AlexNet`\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        # 第一层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(inputs, [5, 5], 64, [1, 1], padding='VALID', scope='conv1')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第二层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool1')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第三层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(net, [5, 5], 64, [1, 1], scope='conv2')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第四层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool2')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 将矩阵拉长成向量\n",
    "        net = tf.reshape(net, [-1, 6*6*64])\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第五层是全连接层, 输出个数为384\n",
    "        net = fc(net, 384, scope='fc3')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第六层是全连接层, 输出个数为192\n",
    "        net = fc(net, 192, scope='fc4')\n",
    "        print(net.get_shape().as_list())\n",
    "        # 第七层是全连接层, 输出个数为10, 注意这里不要使用激活函数\n",
    "        net = fc(net, 10, scope='fc5', act=tf.identity)\n",
    "        print(net.get_shape().as_list())\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 alexnet 构建训练和测试的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 32, 32, 3]\n",
      "[64, 28, 28, 64]\n",
      "[64, 13, 13, 64]\n",
      "[64, 13, 13, 64]\n",
      "[64, 6, 6, 64]\n",
      "[64, 2304]\n",
      "[64, 384]\n",
      "[64, 192]\n",
      "[64, 10]\n"
     ]
    }
   ],
   "source": [
    "###  训练数据预测一次\n",
    "train_out = alexnet(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 32, 32, 3]\n",
      "[64, 28, 28, 64]\n",
      "[64, 13, 13, 64]\n",
      "[64, 13, 13, 64]\n",
      "[64, 6, 6, 64]\n",
      "[64, 2304]\n",
      "[64, 384]\n",
      "[64, 192]\n",
      "[64, 10]\n"
     ]
    }
   ],
   "source": [
    "## 测试数据 预测一次\n",
    "val_out = alexnet(val_imgs, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "这里真实的 labels 不是一个 one_hot 型的向量, 而是一个数值, 因此我们使用 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义正确率`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('train'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造训练`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from learning import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 4157247324160.0000 acc = 0.0625 (0.0024 / batch)\n",
      "[val]: step 0 loss = 4112877355008.0000 acc = 0.1094\n"
     ]
    }
   ],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
